{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning for genetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline \n",
    "# imports matplotlib as plt and numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', **{'size': 16}) # font size for text on plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description\n",
    "\n",
    "* `data/athaliana_small.X.txt` is the design matrix. As many rows as samples, as many columns as SNPs\n",
    "* the SNPs are given (in order) in `data/athaliana_small.snps.txt`. \n",
    "* the samples are given (in order) in `data/athaliana.samples.txt`.\n",
    "\n",
    "* the transformed phenotypes are given in `data/athaliana.4W.pheno` and `data/athaliana.2W.pheno`. The first column is the sample's ID, and the second the phenotype.\n",
    "\n",
    "* `data/athaliana.candidates.txt` contains a list of _A. thaliana_ genes known or strongly suspected to be associated with flowering times.\n",
    "\n",
    "* the feature network is in `data/athaliana_small.W.txt`. It has been saved as 3 arrays, corresponding to the row, col, and data attributes of a [scipy.sparse coo_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "We will start by working without the feature network, on the 2W phenotype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SNP names\n",
    "with open('data/athaliana_small.snps.txt') as f:\n",
    "    snp_names = f.readline().split()\n",
    "    f.close()\n",
    "print(len(snp_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the design matrix -- this can take time!\n",
    "X = np.loadtxt('data/athaliana_small.X.txt',  # file names\n",
    "               dtype = 'int') # values are integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1307 samples and 9 419 SNPs.\n",
    "\n",
    "The 2W phenotype is not available for all samples. \n",
    "\n",
    "#### Load the sample names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(np.loadtxt('data/athaliana.samples.txt', # file names\n",
    "                         dtype = int)) # values are integers\n",
    "print(len(samples))\n",
    "print(samples[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the 2W phenotype data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/athaliana.2W.pheno', # file name\n",
    "                 header = None, # columns have no header\n",
    "                 delim_whitespace = True) # columns are separated by white space\n",
    "\n",
    "# Create vector of sample IDs\n",
    "samples_with_phenotype = list(df[0])\n",
    "print(len(samples_with_phenotype), \"samples have a phenotype\")\n",
    "\n",
    "# Create vector of phenotypes\n",
    "y_2W = np.array(df[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict X to the samples with a 2W phenotype, in correct order\n",
    "# X_2W[i] = X[samples.index(samples_with_phenotype[i])]\n",
    "X_2W = X[np.array([samples.index(sample_id) \\\n",
    "                   for sample_id in samples_with_phenotype]), :]\n",
    "print(X_2W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the list of candidate genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/athaliana.candidates.txt') as f:\n",
    "    candidate_genes = f.readline().split()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the snps to gene mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_by_snp = {} # key: SNP, value = [genes in/near which this SNP is]\n",
    "with open('data/athaliana.snps_by_gene.txt') as f:\n",
    "    for line in f:\n",
    "        ls = line.split()\n",
    "        gene_id = ls[0]\n",
    "        for snp_id in ls[1:]:\n",
    "            if not snp_id in genes_by_snp:\n",
    "                genes_by_snp[snp_id] = []\n",
    "            genes_by_snp[snp_id].append(gene_id) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data in a train and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set aside a test set, containing 20% of our samples, on which to evaluate the quality of our predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2W_tr, X_2W_te, y_2W_tr, y_2W_te = \\\n",
    "    model_selection.train_test_split(X_2W, y_2W, test_size = 0.2, random_state = 17)\n",
    "print(X_2W_tr.shape, X_2W_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic net\n",
    "\n",
    "One solution to make the lasso more stable is to use a combination of the l1 and l2 regularizations.\n",
    "\n",
    "We are now minimizing the loss + a linear combination of an l1-norm and an l2-norm over the regression weights. This imposes sparsity, but encourages correlated features to be selected together, where the lasso would tend to pick only one (at random) of a group of correlated features.\n",
    "\n",
    "The elastic net is implemented in scikit-learn's [linear_model.ElasticNet](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 3e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_lasso, coefs_lasso, _ = linear_model.lasso_path(X_2W_tr[:, :], y_2W_tr, eps = eps, fit_intercept = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_enet, coefs_enet, _ = linear_model.enet_path(\n",
    "    X_2W_tr[:, :], y_2W_tr, eps = eps, l1_ratio = 0.95, fit_intercept = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "import matplotlib.colors as mcolors\n",
    "colors = cycle(list(mcolors.TABLEAU_COLORS.keys()))\n",
    "figure(figsize = (10, 6))\n",
    "neg_log_alphas_lasso = -np.log10(alphas_lasso)\n",
    "neg_log_alphas_enet = -np.log10(alphas_enet)\n",
    "for coef_l, coef_e, c in zip(coefs_lasso, coefs_enet, colors):\n",
    "    l1 = plt.plot(neg_log_alphas_lasso, coef_l, c = c)\n",
    "    l2 = plt.plot(neg_log_alphas_enet, coef_e, linestyle = '--', c = c)\n",
    "\n",
    "plt.xlabel('-Log(alpha)')\n",
    "plt.ylabel('coefficients')\n",
    "plt.title('Lasso and Elastic-Net Paths')\n",
    "plt.legend((l1[-1], l2[-1]), ('Lasso', 'Elastic-Net'), loc='lower left')\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q: Compared to the lasso, what is the effect of the elastic-net on the coefficients?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit lastic-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters grid\n",
    "alphas = np.logspace(-4., 1., num = 15)\n",
    "ratios = np.linspace(0.5, 1., num = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q: Define the elastic net model (call it `model_l1l2`) using the functions `ElasticNet` and `GridSearchCV`.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 4))\n",
    "plt.scatter(range(p), # x = SNP position\n",
    "            model_l1l2.coef_)  # y = regression weights\n",
    "\n",
    "plt.xlabel(\"SNP\")\n",
    "plt.ylabel(\"elastic net regression weight\")\n",
    "plt.xlim([0, p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How can you interpret these results? How many SNPs contribute to explaining the phenotype?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%d SNPs selected,\" % \\\n",
    "    np.nonzero(model_l1l2.coef_)[0].shape)\n",
    "\n",
    "candidate_genes_hit = set([])\n",
    "num_snps_in_candidate_genes = 0\n",
    "for snp_idx in np.nonzero(model_l1l2.coef_)[0]:\n",
    "    for gene_id in genes_by_snp[snp_names[snp_idx]]:\n",
    "        if gene_id in candidate_genes and gene_id not in candidate_genes_hit:\n",
    "            candidate_genes_hit.add(gene_id)\n",
    "            num_snps_in_candidate_genes += 1\n",
    "\n",
    "print(\"of which %d are in %d candidate genes\" % (num_snps_in_candidate_genes, \n",
    "                                                          len(candidate_genes_hit)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive power "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2W_l1l2_pred = model_l1l2.predict(X_2W_te)\n",
    "\n",
    "print(\"Percentage of variance explained (using %d SNPs): %.2f\" % \\\n",
    "     (np.nonzero(model_l1l2.coef_)[0].shape[0], \n",
    "      metrics.explained_variance_score(y_2W_te, y_2W_l1l2_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (4, 4))\n",
    "plt.scatter(y_2W_te, y_2W_l1l2_pred)\n",
    "\n",
    "plt.xlabel(\"true phenotype\")\n",
    "plt.ylabel(\"prediction\")\n",
    "plt.xlim([np.min(y_2W_te) - 0.05, np.max(y_2W_te) + 0.05])\n",
    "plt.ylim([np.min(y_2W_te) - 0.05, np.max(y_2W_te) + 0.05])\n",
    "plt.axline(xy1 = [0, 0], slope = 1, c = \"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-task feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Repeat the previous analysis for the 4W phenotype. It is very similar to the 2W phenotype, except that the seeds have been vernelized for 4 weeks. \n",
    "\n",
    "2) It is not unreasonable to expect the genomic regions driving both those phenotypes to be (almost) the same. Use the multi-task version of the Lasso, ENet, or ncLasso algorithms to analyzed both phenotypes simultaneously.\n",
    "\n",
    "Use [sklearn.linear_model.MultiTaskLasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html#sklearn.linear_model.MultiTaskLasso) + [User Guide](http://scikit-learn.org/stable/auto_examples/linear_model/plot_multi_task_lasso_support.html)\n",
    "\n",
    "3) __Q: Is it the same \"multi-task\" setting as the one in the lecture? What is the difference?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load 4W and 2W phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2W = X[np.array([samples.index(sample_id) \\\n",
    "                   for sample_id in samples_with_phenotype]), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2W = pd.read_csv('data/athaliana.2W.pheno', # file name\n",
    "                 header = None, # columns have no header\n",
    "                 delim_whitespace = True) # columns are separated by white space\n",
    "\n",
    "# Create vector of sample IDs\n",
    "samples_with_phenotype_2W = list(df[0])\n",
    "print(len(samples_with_phenotype_2W), \"samples have a phenotype\")\n",
    "\n",
    "# Create vector of phenotypes\n",
    "y_2W = np.array(df_2W[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt('data/athaliana_small.X.txt',  # file names\n",
    "               dtype = 'int') # values are integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4W = pd.read_csv('data/athaliana.4W.pheno', # file name\n",
    "                 header = None, # columns have no header\n",
    "                 delim_whitespace = True) # columns are separated by white space\n",
    "\n",
    "# Create vector of sample IDs\n",
    "samples_with_phenotype_4W = list(df_4W[0])\n",
    "print(len(samples_with_phenotype_4W), \"samples have a 4W phenotype\")\n",
    "\n",
    "# Create vector of phenotypes\n",
    "y_4W = np.array(df_4W[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_4W = X[np.array([samples.index(sample_id) \\\n",
    "                   for sample_id in samples_with_phenotype_4W]), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_4W_tr, X_4W_te, y_4W_tr, y_4W_te = \\\n",
    "    model_selection.train_test_split(X_4W, y_4W, test_size = 0.2, random_state = 17)\n",
    "print(X_4W_tr.shape, X_4W_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary analysis\n",
    "__Q: Why would we expect the 2W and 4W phenotypes to share many explanatory SNPs? How could you verify this using Manhattan plots?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer:\n",
    "## Tip: compute pvalues for both 2W and 4W (using association test, cf practical1.ipynb).\n",
    "## Tip: plot both these p-values and comment the visual result.\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possible visualization is to plot both pvalues ax xy-coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now plot the pvalues of 2W and 4W:\n",
    "figure(figsize = (5, 5))\n",
    "#plt.scatter(-np.log10(pvalues_2W), -np.log10(pvalues_4W))\n",
    "plt.xlabel('2W'); plt.ylabel('4W')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q: What conclusion can you draw? Do the 2W and 4W phenotypes seem to be linked to the same genome loci?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define design matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample with *both* 2W and 4W phenotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_with_phenotype_both = list(set(samples_with_phenotype_2W).intersection(samples_with_phenotype_4W))\n",
    "print(len(samples_with_phenotype_both), \"samples have both phenotypes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_both = X[np.array([samples.index(sample_id) for sample_id in samples_with_phenotype_both]), :]\n",
    "del X # You can delete X now if you want, to free space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter, order, and merge the two phenotype data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter out the samples that do not have both phenotypes\n",
    "df_4W_filtered = df_4W[df_4W[0].isin(samples_with_phenotype_both)]\n",
    "df_2W_filtered = df_2W[df_2W[0].isin(samples_with_phenotype_both)]\n",
    "## Set sample names as the DataFrame index name\n",
    "df_2W_filtered.index = list(df_2W_filtered[0])\n",
    "df_4W_filtered.index = list(df_4W_filtered[0])\n",
    "## Reorder both datasets so as to have the phenotypes in the same order\n",
    "df_2W_ordered = df_2W_filtered.reindex(samples_with_phenotype_both)\n",
    "df_4W_ordered = df_4W_filtered.reindex(samples_with_phenotype_both)\n",
    "## Define the phenotype matrix\n",
    "y_both = np.array([df_2W_ordered[1], df_4W_ordered[1]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split in train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_both_tr, X_both_te, y_both_tr, y_both_te = \\\n",
    "    model_selection.train_test_split(X_both, y_both, test_size = 0.2, random_state = 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_both.shape, y_both_tr.shape, y_both_te.shape)\n",
    "print(X_both.shape, X_both_tr.shape, X_both_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Multi-task lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q:__ Run MultiTaskLasso (and call the object `model_mt_l1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.explained_variance_score(y_both_te, model_mt_l1.best_estimator_.predict(X_both_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mt_l1_2 = linear_model.MultiTaskLassoCV(fit_intercept = True, max_iter = 6000, \n",
    "                                              cv = 5, random_state = 18).fit(X_both_tr, y_both_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.explained_variance_score(y_both_te, model_mt_l1_2.predict(X_both_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 5))\n",
    "#plt.spy(ml_lasso_cv.best_estimator_.coef_)\n",
    "plt.scatter(range(p), model_mt_l1.best_estimator_.coef_[0, :])\n",
    "plt.scatter(range(p), model_mt_l1.best_estimator_.coef_[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_both_l1_pred = model_mt_l1.best_estimator_.predict(X_both_te)\n",
    "\n",
    "print(\"Percentage of variance explained (using %d SNPs): %.2f\" % \\\n",
    "     (np.nonzero(model_mt_l1.best_estimator_.coef_)[0].shape[0], \n",
    "      metrics.explained_variance_score(y_both_te, y_both_l1_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Multi-task elastic-net\n",
    "See the (https://scikit-learn.org/stable/modules/linear_model.html#multi-task-elastic-net)[user guide] and (API)[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskElasticNet.html#sklearn.linear_model.MultiTaskElasticNet]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q: Do the same as before, but with (multi-task) elastic net instead!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network-constrained lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_saved = np.loadtxt('data/athaliana_small.W.txt') # adjacency matrix is in coordinate (or triplet) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create adjacency matrix in scipy.sparse.coo_matrix format\n",
    "W = sparse.coo_matrix((w_saved[2, :], (np.array(w_saved[0, :], dtype = int), \n",
    "                                       np.array(w_saved[1, :], dtype = int))),\n",
    "                      shape = (p, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the transposed incidence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q: The incidence matrix of a graph? What is it again? You may refer to Wikipedia, or the paper `Network-constrained regularization and variable selection\n",
    "for analysis of genomic data`__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.zeros((p, ))\n",
    "for vertex in W.row:\n",
    "    degrees[vertex] += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_ix = 0\n",
    "tim = sparse.lil_matrix((w_saved[0].shape[0] // 2, p))\n",
    "for ix, edge in enumerate(w_saved[2, :]):\n",
    "    if w_saved[0, ix] < w_saved[1, ix]: # counts each edge-vertex link only once\n",
    "        tim[row_ix, w_saved[0, ix]] = np.sqrt(edge / degrees[W.row[ix]])\n",
    "        tim[row_ix, w_saved[1, ix]] = -np.sqrt(edge / degrees[W.row[ix]])\n",
    "        row_ix += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ncLasso class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import base, linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network-constrained lasso is just a disguised lasso! By defining a modified design matrix X_new and output vector y_new, we can solve the network-constrained lasso problem using the usual `linear_model.Lasso` function.\n",
    "\n",
    "__Q: Go on the [paper introducing network-constrained lasso](https://watermark.silverchair.com/btn081.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAArowggK2BgkqhkiG9w0BBwagggKnMIICowIBADCCApwGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMDe8GLDpW7r7m-akpAgEQgIICbfpkYq0Lr-mTYblz7p4QoXY1GEQWv79yv4os3nGyI9V7jpY4hitfcXBPLHoskH-EGIbvKa_R4cfMBrKOPpB3v_CJDz_fjBCaLC2MHeW9R4hXKbvIWQETPQbBU5mUBbZ9xgi4qaFcumEW8huUcwZoOIH2lWE2FshQmDNGsc7M0sQmVUPh2I6EQ3kh3lDA6_MGVwJ2rFgc7celLZDPOsVCMNQDf33ZcZTxZukzZINIndNpt6TwP4OCBWaSgRnBChfic9mhwAqxaZ6if-mGGMbgaqCkoWo5LIkOd6uqAAOoD7G3hzDnqimAauDAaUEgK2UTIV6_xFLC3lmjuGWoXrih4pM_6vsgTjOVj59iaPDbd6a0IpnpmzkJdlG3eNmvah4UPv02MGJZvDda0GNrySaTSvNrGtkzXgGpIe_nIQW75H7xI0n8yRvtLPQ2aph22U2LvSpBAyxw23-oW13K5PD9irGviaJ9pNbQShxvFd_tFToCy0J2lnzzgp-JIsQJlJzXbwtePqXH3VgakG9YjCRapJCU0fC8gnbwzjUp-g-P2NpjBYx7Ktq5TDY19vC1bFUOqsBAJ3cmQpCES5bc3llSHRASkmQxl7h9VpGWGm5xv98_hHdl3Ee3G63XUIBit5717BWXP-nlLM_4JrnDXbXe2MJjViSQCW7YNcSFtQZ3DlsGM7Ygoz-pSs5ZbTUDOQq8AO6nniBKekcBnWSV5zFHbZsJaCKccMySMYxyyeM-9oS_YLZ09Yo4e9cGO8HwlDtV6bCCoz714n4oX0XUqDFFv8Ux5qXgvdEMGTYUZl1cUBLvO8Is2nWAq6cP6Hn86w) and fill in the missing line using Lemma 1.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ncLasso(base.BaseEstimator, base.RegressorMixin):\n",
    "    def __init__(self, transposed_incidence = None, lambda1 = 1.0, lambda2 = 1.0):\n",
    "        self.transposed_incidence = transposed_incidence # sparse matrix\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        \n",
    "    def fit(self, X, y, max_iter = 1000):       \n",
    "        alpha = self.lambda1 / (np.sqrt(self.lambda2 + 1.))\n",
    "        self.lasso = linear_model.Lasso(fit_intercept = True, \n",
    "                                        alpha = alpha, max_iter = max_iter)\n",
    "        y_new = np.hstack((y, np.zeros((self.transposed_incidence.shape[0], ))))\n",
    "        X_new = 1 / np.sqrt(self.lambda2 + 1) * sparse.vstack(\n",
    "            ## Missing line\n",
    "        )\n",
    "        self.lasso.fit(X_new, y_new)\n",
    "        self.coef_ = self.lasso.coef_[:X.shape[1]] / np.sqrt(self.lambda2 + 1)\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def predict(self, X, y = None):\n",
    "        return self.lasso.predict(X)\n",
    "    \n",
    "    \n",
    "    def score(self, X, y = None):\n",
    "        return self.lasso.score(X, y)                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run network-constrained lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nclasso = ncLasso(tim, 0.00005, 0.00001)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "## DO NOT RUN (takes two hours to run)\n",
    "# model_nclasso.fit(X_2W_tr, y_2W_tr, max_iter = 2000)\n",
    "## END DO NOT RUN\n",
    "end = time.time()\n",
    "#print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "## DO NOT RUN\n",
    "#with open('model_nclasso.obj', 'wb') as f:\n",
    "#    pickle.dump(model_nclasso, f, pickle.HIGHEST_PROTOCOL)\n",
    "## END DO NOT RUN\n",
    "\n",
    "with open('model_nclasso.obj', 'rb') as f:\n",
    "    model_nclasso = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 4))\n",
    "plt.scatter(range(p), # x = SNP position\n",
    "            model_nclasso.coef_)  # y = regression weights\n",
    "\n",
    "plt.xlabel(\"SNP\")\n",
    "plt.ylabel(\"elastic net regression weight\")\n",
    "plt.xlim([0, p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%d SNPs selected,\" % \\\n",
    "    np.nonzero(model_nclasso.coef_)[0].shape)\n",
    "\n",
    "candidate_genes_hit = set([])\n",
    "num_snps_in_candidate_genes = 0\n",
    "for snp_idx in np.nonzero(model_nclasso.coef_)[0]:\n",
    "    for gene_id in genes_by_snp[snp_names[snp_idx]]:\n",
    "        if gene_id in candidate_genes:\n",
    "            candidate_genes_hit.add(gene_id)\n",
    "            num_snps_in_candidate_genes += 1\n",
    "\n",
    "print(\"%d SNPs are in %d candidate genes\" % (num_snps_in_candidate_genes, \n",
    "                                                          len(candidate_genes_hit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ncl1_pred = model_nclasso.predict(X_2W_te)\n",
    "print(metrics.explained_variance_score(y_2W_te, y_ncl1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further\n",
    "You can also used methods developped specifically for SNP detections! \n",
    "\n",
    "For instance SConES (Azencott et al 2013) has a publicly available implementation as an R package (https://github.com/hclimente/martini) or in python (https://github.com/chagaz/sfan)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
