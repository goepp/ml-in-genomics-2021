{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing for the ML for genetic data notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Genotype and phenotype data were downloaded as PLINK files from [Easy GWAS](https://easygwas.ethz.ch/data/public/dataset/view/1/)\n",
    "* Protein-protein interaction data was downloaded from [TAIR](https://www.arabidopsis.org/download/index-auto.jsp?dir=%2Fdownload_files%2FProteins%2FProtein_interaction_data)\n",
    "* Gene positions were downloaded from [TAIR](https://www.arabidopsis.org/download/index-auto.jsp?dir=%2Fdownload_files%2FGenes%2FTAIR9_genome_release%2Ftair9_gff3) \n",
    "\n",
    "The genome build is TAIR9.\n",
    "\n",
    "## Cleaning genotype data\n",
    "* Converted PLINK genotypes to a 0/1 encoding\n",
    "\n",
    "```\n",
    "   plink --file genotype --recode A --out arabidopsis\n",
    "```\n",
    "\n",
    "* Common pre-processing step in Arabidopsis GWAS studies: remove SNPS with a minor allele frequency of less than 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only use SNPs in/near candidate genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of all SNPs in the data\n",
    "with open('data/athaliana.raw') as f:\n",
    "    header = f.readline()\n",
    "    snp_names = header.split()[6:]\n",
    "    f.close()\n",
    "print(len(snp_names), snp_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of candidate genes\n",
    "import h5py\n",
    "f = h5py.File('data/arabidopsis_processed_data/FT_cand_genes.hd5', 'r')\n",
    "candidate_genes = set(f['gene_id'])\n",
    "print(len(candidate_genes), list(candidate_genes)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate genes positions (not trusting those in the hd5 file to be in the TAIR9 build)\n",
    "chromosomes = {}\n",
    "window = 20000\n",
    "\n",
    "with open('data/TAIR9_GFF3_genes.gff') as f:\n",
    "    for line in f:\n",
    "        ls = line.split()\n",
    "        if ls[2] == 'protein':\n",
    "            if ls[0] in ['ChrC', 'ChrM']:\n",
    "                continue\n",
    "            ccc = int(ls[0][3])\n",
    "            beg = int(ls[3])\n",
    "            end = int(ls[4])\n",
    "            protein_name = ls[8].split(\".\")[0][3:]\n",
    "            if protein_name in candidate_genes:\n",
    "                if not chromosomes.has_key(ccc):\n",
    "                    chromosomes[ccc] = {}\n",
    "                chromosomes[ccc][(beg-window)] = [(end+window), protein_name]\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromosomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chromosomes), len(chromosomes[1]))\n",
    "print(chromosomes[1].keys()[:10])\n",
    "print(chromosomes[1][chromosomes[1].keys()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort candidate gene positions (for easier search)\n",
    "sorted_begins = []\n",
    "chromosomes_ids = chromosomes.keys()\n",
    "chromosomes_ids.sort()\n",
    "print(chromosomes_ids)\n",
    "for ccc in chromosomes_ids:\n",
    "    begins = chromosomes[ccc].keys()\n",
    "    begins.sort()\n",
    "    print(ccc, len(begins))\n",
    "    sorted_begins.append(begins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_snps = [] # list of indices (in snp_names) of SNPs in/near candidate genes\n",
    "snps_in_genes = {} # key = SNP name, value = [index (in snp_names), set of proteins]\n",
    "snps_by_genes = {} # key = protein name, value = list of SNP names.\n",
    "\n",
    "for snp_idx, snp in enumerate(snp_names):\n",
    "    ccc = int(snp[3])\n",
    "    pos = int(snp.split(\"_\")[1])\n",
    "    begins = sorted_begins[(ccc-1)]\n",
    "    \n",
    "    # list all possible begin positions (those that are before pos)\n",
    "    possible_begs = []\n",
    "    for beg in begins:\n",
    "        if beg <= pos:\n",
    "            possible_begs.append(beg)\n",
    "        else:\n",
    "            break \n",
    "            \n",
    "    proteins = set([])\n",
    "    # reverse the list of possible begin positions\n",
    "    possible_begs.reverse()\n",
    "    for beg in possible_begs:\n",
    "        # look for end positions that are after pos\n",
    "        end, protein_name = chromosomes[ccc][beg]\n",
    "        if end >= pos:\n",
    "            if not snps_in_genes.has_key(snp):\n",
    "                snps_in_genes[snp] = [snp_idx]\n",
    "            proteins.add(protein_name)\n",
    "        else:\n",
    "            # now both begin and end are before pos, stop looking\n",
    "            break\n",
    "                \n",
    "    if snps_in_genes.has_key(snp):\n",
    "        # this SNP is in/near a candidate gene, keep processing\n",
    "        snps_in_genes[snp].append(proteins)\n",
    "        for protein_name in proteins:\n",
    "            if not snps_by_genes.has_key(protein_name):\n",
    "                snps_by_genes[protein_name] = []\n",
    "            snps_by_genes[protein_name].append(snp)\n",
    "                \n",
    "        cand_snps.append(snp_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cand_snps), cand_snps[:10])\n",
    "print(len(snps_in_genes), snps_in_genes.keys()[:10])\n",
    "print(snps_in_genes[snps_in_genes.keys()[0]])\n",
    "print(len(snps_by_genes), snps_by_genes.keys()[:10])\n",
    "print(snps_by_genes[snps_by_genes.keys()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_snps.sort()\n",
    "print(len(set(cand_snps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by minor allele frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count minor allele frequency\n",
    "with open('data/athaliana.raw') as f:\n",
    "    header = f.readline()\n",
    "    snps_ids = header.split()[6:]\n",
    "    snps_freq_dict = {} # key: snp name, value: number of individuals with this SNP's minor allele\n",
    "    num_samples = 0\n",
    "    # initialize snp counts\n",
    "    for idx in cand_snps:\n",
    "        snps_freq_dict[idx] = 0\n",
    "    # count minor alleles\n",
    "    for line in f:\n",
    "        num_samples += 1\n",
    "        ls = line.split()[6:]\n",
    "        for snp_idx in cand_snps:\n",
    "            if int(ls[snp_idx]):\n",
    "                snps_freq_dict[snp_idx] += 1\n",
    "    f.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_samples)\n",
    "print(len(snps_freq_dict), snps_freq_dict.keys()[:10])\n",
    "print(snps_freq_dict[snps_freq_dict.keys()[20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maf threshold = 10%\n",
    "threshold = 0.1 * float(num_samples)\n",
    "\n",
    "keep_snp_names = [] \n",
    "keep_snp_indices = []\n",
    "\n",
    "for snp_idx, snp_id in enumerate(snp_names):\n",
    "    if snps_freq_dict.has_key(snp_idx):\n",
    "        if snps_freq_dict[snp_idx] > threshold:\n",
    "            # keep SNP\n",
    "            keep_snp_names.append(snp_id)\n",
    "            keep_snp_indices.append(snp_idx)\n",
    "            \n",
    "print(len(keep_snp_indices), keep_snp_indices[:10], keep_snp_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/athaliana_small.snps.txt', 'w') as f:\n",
    "    f.write(\"%s\\n\" % \" \".join(keep_snp_names))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "with open('data/athaliana.raw') as f:\n",
    "    f.readline() # header\n",
    "    with open('data/athaliana_small.X.txt', 'w') as g:\n",
    "        for line in f:\n",
    "            samples.append(line.split()[0])\n",
    "            snps = np.array(line.split()[6:])\n",
    "            # only keep SNPs with index in keep_snp_indices\n",
    "            # and convert \"2\" into \"1\"\n",
    "            g.write(\"%s\\n\" % str.replace(\" \".join(snps[keep_snp_indices]), '2', '1'))\n",
    "    f.close()\n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/athaliana.samples.txt', 'w') as g:\n",
    "    g.write(\" \".join(samples))\n",
    "    g.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/athaliana_small.snps.txt') as f:\n",
    "    snp_names = f.readline().split()\n",
    "    f.close()\n",
    "print(len(snp_names), snp_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(np.loadtxt('data/athaliana.samples.txt', # file names\n",
    "                         dtype=int)) # values are integers\n",
    "print(len(samples), samples[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the design matrix \n",
    "X = np.loadtxt('data/athaliana_small.X.txt',  # file names\n",
    "               dtype='int') # values are integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "* Create a sparse matrix.\n",
    "* Create edges between all neighboring SNPs (in the genetic sequence), with a weight of 0.01.\n",
    "* Create edges between all pairs of SNPs that are in the same gene.\n",
    "* Create edges between all pairs of SNPs that are in two interacting genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = len(keep_snp_indices)\n",
    "print(p)\n",
    "W = sparse.lil_matrix(sparse.eye(p, k=1)*0.01) # connect each SNP to the next one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SNP index in X from its name\n",
    "snp_index = {}\n",
    "for snp_idx, snp_name in enumerate(keep_snp_names):\n",
    "    snp_index[snp_name] = snp_idx\n",
    "print(len(snp_index), snp_index.keys()[:10], snp_index[snp_index.keys()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gene membership\n",
    "for snp_list in snps_by_genes.values():\n",
    "    if not len(snp_list):\n",
    "        break\n",
    "    for ix1, snp1 in enumerate(snp_list):\n",
    "        if snp_index.has_key(snp1):\n",
    "            # We kept that SNP\n",
    "            for snp2 in snp_list[ix1+1:]:\n",
    "                if snp_index.has_key(snp2):\n",
    "                    W[snp_index[snp1], snp_index[snp2]] = 1.\n",
    "                    W[snp_index[snp2], snp_index[snp1]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protein-protein interactions\n",
    "ppi_set = set([])\n",
    "with open('data/TairProteinInteraction.20090527.txt') as f:\n",
    "    f.readline() # header\n",
    "    for line in f:\n",
    "        ls = line.split()\n",
    "        if ls[0] < ls[2]:\n",
    "            ppi = (ls[0], ls[2])\n",
    "        else:\n",
    "            ppi = (ls[2], ls[0])\n",
    "        ppi_set.add(ppi)\n",
    "print(len(ppi_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ppi in ppi_set:\n",
    "    if snps_by_genes.has_key(ppi[0]):\n",
    "        for snp1 in snps_by_genes[ppi[0]]:        \n",
    "            if snp_index.has_key(snp1):\n",
    "                if snps_by_genes.has_key(ppi[1]):\n",
    "                    for snp2 in snps_by_genes[ppi[1]]:\n",
    "                        if snp_index.has_key(snp2):\n",
    "                            W[snp_index[snp1], snp_index[snp2]] = 1.\n",
    "                            W[snp_index[snp2], snp_index[snp1]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1291643./(9419*9419)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = sparse.coo_matrix(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([W.row, W.col, W.data])\n",
    "np.savetxt('data/athaliana_small.W.txt', A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_saved = np.loadtxt('data/athaliana_small.W.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = sparse.coo_matrix((w_saved[2, :], (np.array(w_saved[0, :], dtype=int), \n",
    "                                       np.array(w_saved[1, :], dtype=int))), shape=(p, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check incidence matrix can be built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute node degrees \n",
    "degrees = np.zeros((p, ))\n",
    "for vertex in W.row:\n",
    "    degrees[vertex] += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tim = sparse.lil_matrix((W.row.shape[0], p))\n",
    "for ix, edge in enumerate(W.data):\n",
    "    tim[ix, W.row[ix]] = np.sqrt(edge / degrees[W.row[ix]])\n",
    "    tim[ix, W.col[ix]] = - np.sqrt(edge / degrees[W.col[ix]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check ncLasso runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del W, degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/old/athaliana.2W.pheno', # file name\n",
    "                 header=None, # columns have no header\n",
    "                 delim_whitespace=True) # columns are separated by white space\n",
    "\n",
    "# Create vector of sample IDs\n",
    "samples_with_phenotype = list(df[0])\n",
    "print(len(samples_with_phenotype), \"samples have a phenotype\")\n",
    "\n",
    "# Create vector of phenotypes\n",
    "y_2W = np.array(df[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(samples_with_phenotype), samples_with_phenotype[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict X to the samples with a 2W phenotype, in correct order\n",
    "# X_2W[i] = X[samples.index(samples_with_phenotype[i])]\n",
    "X_2W = X[np.array([samples.index(sample_id) \\\n",
    "                   for sample_id in samples_with_phenotype]), :]\n",
    "n, p = X_2W.shape\n",
    "print(n, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can delete X now to free space\n",
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import base, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ncLasso(base.BaseEstimator, base.RegressorMixin):\n",
    "    def __init__(self, transposed_incidence=None, lambda1=1.0, lambda2=1.0):\n",
    "        self.transposed_incidence = transposed_incidence # sparse matrix\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        \n",
    "        alpha = self.lambda1/(np.sqrt(self.lambda2+1.))\n",
    "        self.lasso = linear_model.Lasso(fit_intercept=True, alpha=alpha)\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        print(self.lambda1)\n",
    "        print(self.lambda2)\n",
    "        \n",
    "        y_new = np.hstack((y, np.zeros((self.transposed_incidence.shape[0], ))))\n",
    "        print(y_new.shape, X.shape)\n",
    "        X_new = 1/(np.sqrt(self.lambda2+1)) * sparse.vstack((X, np.sqrt(self.lambda2)*\\\n",
    "                                                    self.transposed_incidence))\n",
    "        \n",
    "        \n",
    "        self.lasso.fit(X_new, y_new)\n",
    "        self.coef_ = self.lasso.coef_[:X.shape[1]]/(np.sqrt(self.lambda2+1))\n",
    "        \n",
    "        \n",
    "    def predict(self, X, y=None):\n",
    "        return self.lasso.predict(X)\n",
    "    \n",
    "    \n",
    "    def score(self, X, y=None):\n",
    "        return self.lasso.score(X, y)                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [0.001]# np.logspace(-3., 2., num=6)\n",
    "l2 = [0.1] #np.logspace(0., 2., num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclasso = ncLasso(transposed_incidence=tim)\n",
    "print nclasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nc = model_selection.GridSearchCV(nclasso, param_grid={'lambda1': l1, \n",
    "                                                             'lambda2': l2}, \n",
    "                                        scoring='explained_variance')\n",
    "model_nc.fit(X_2W, y_2W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(range(p), # x = SNP position\n",
    "            model_nc.coef_)  # y = regression weights\n",
    "\n",
    "plt.xlabel(\"SNP\")\n",
    "plt.ylabel(\"lasso regression weight\")\n",
    "plt.xlim([0, p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count minor allele frequency\n",
    "with open('data/athaliana.raw') as f:\n",
    "    header = f.readline()\n",
    "    snps_ids = header.split(\" \")[6:]\n",
    "    snps_freq_dict = {} # key: snp name, value: number of individuals with this SNP's minor allele\n",
    "    num_samples = 0\n",
    "    for idx in range(len(snps_ids)):\n",
    "        snps_freq_dict[idx] = 0\n",
    "    for line in f:\n",
    "        num_samples += 1\n",
    "        for idx, snp in enumerate(line.split(\" \")[6:]):\n",
    "            if int(snp):\n",
    "                snps_freq_dict[idx] += 1\n",
    "    f.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maf threshold = 10%\n",
    "threshold = 0.1 * float(num_samples)\n",
    "    \n",
    "new_header = []\n",
    "snps_pass_maf_idx = []\n",
    "for idx in range(len(snps_ids)):\n",
    "    if snps_freq_dict[idx] > threshold:\n",
    "        # keep SNP\n",
    "        new_header.append(snps_ids[idx])\n",
    "        snps_pass_maf_idx.append(idx)\n",
    "        \n",
    "print(len(new_header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/athaliana.snps.txt', 'w') as g:\n",
    "    g.write(\" \".join(new_header))\n",
    "    g.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "* Map SNPS to genes using a 20kbp window.\n",
    "* For the purpose of this lab, we want to be able to run all methods quickly. We selected 20 000 SNPs at random among those that are mapped to genes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/athaliana.snps.txt') as f:\n",
    "    snps_pass_maf = f.readline().split()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(snps_pass_maf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromosomes = {}\n",
    "window = 20000\n",
    "with open('data/TAIR9_GFF3_genes.gff') as f:\n",
    "    for line in f:\n",
    "        ls = line.split()\n",
    "        if ls[2] == 'protein':\n",
    "            if ls[0] in ['ChrC', 'ChrM']:\n",
    "                continue\n",
    "            ccc = int(ls[0][3])\n",
    "            beg = int(ls[3])\n",
    "            end = int(ls[4])\n",
    "            protein_name = ls[8].split(\".\")[0][3:]\n",
    "            if not chromosomes.has_key(ccc):\n",
    "                chromosomes[ccc] = {}\n",
    "            chromosomes[ccc][(beg-window)] = [(end+window), protein_name]\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_begins = []\n",
    "chromosomes_ids = chromosomes.keys()\n",
    "chromosomes_ids.sort()\n",
    "print(chromosomes_ids)\n",
    "for ccc in chromosomes_ids:\n",
    "    begins = chromosomes[ccc].keys()\n",
    "    begins.sort()\n",
    "    print ccc, len(begins)\n",
    "    sorted_begins.append(begins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File('data/arabidopsis_processed_data/FT_cand_genes.hd5', 'r')\n",
    "candidate_genes = set(f['gene_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_snps = [] # list of indices (in snps_pass_maf) of candidate SNPs\n",
    "gene_snps = [] # list of indices (in snps_pass_maf) of SNPs in/near genes\n",
    "snps_in_genes = {} # key = SNP name, value = [index (in snps_pass_maf), set of proteins]\n",
    "snps_by_genes = {} # key = protein name, value = list of SNP names.\n",
    "\n",
    "\n",
    "for idx_in_snps_pass_maf, snp in enumerate(snps_pass_maf):\n",
    "    ccc = int(snp[3])\n",
    "    pos = int(snp.split(\"_\")[1])\n",
    "    begins = sorted_begins[(ccc-1)]\n",
    "    \n",
    "    # list all possible begin positions (those that are before pos)\n",
    "    possible_begs = []\n",
    "    for beg in begins:\n",
    "        if beg <= pos:\n",
    "            possible_begs.append(beg)\n",
    "        else:\n",
    "            break \n",
    "            \n",
    "    proteins = set([])\n",
    "    # reverse the list of possible begin positions\n",
    "    possible_begs.reverse()\n",
    "    for beg in possible_begs:\n",
    "        # look for end positions that are after pos\n",
    "        end, protein_name = chromosomes[ccc][beg]\n",
    "        if end >= pos:\n",
    "            if not snps_in_genes.has_key(snp):\n",
    "                snps_in_genes[snp] = [idx_in_snps_pass_maf]\n",
    "            proteins.add(protein_name)\n",
    "        else:\n",
    "            # now both begin and end are before pos, stop looking\n",
    "            break\n",
    "                \n",
    "    if snps_in_genes.has_key(snp):\n",
    "        # this is not an intergenic gene, keep processing\n",
    "        snps_in_genes[snp].append(proteins)\n",
    "        for protein_name in proteins:\n",
    "            if not snps_by_genes.has_key(protein_name):\n",
    "                snps_by_genes[protein_name] = []\n",
    "            snps_by_genes[protein_name].append(snp)\n",
    "                \n",
    "        if len(proteins.intersection(candidate_genes)):\n",
    "            cand_snps.append(idx_in_snps_pass_maf)\n",
    "        else:\n",
    "            gene_snps.append(idx_in_snps_pass_maf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cand_snps), len(gene_snps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(snps_pass_maf[:10], len(snps_pass_maf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices in snps_pass_maf of the SNPs that we are keeping\n",
    "now_keeping = [idx for idx in cand_snps]\n",
    "import random\n",
    "random.shuffle(gene_snps)\n",
    "now_keeping.extend(gene_snps[:10000])\n",
    "now_keeping.sort()\n",
    "\n",
    "# indices in the original SNP list of the SNPs that we are keeping\n",
    "with open('data/athaliana.raw') as f:\n",
    "    all_snps = f.readline().split()[6:]\n",
    "    f.close()\n",
    "    \n",
    "now_keeping_indices = []\n",
    "for idx in now_keeping:\n",
    "    now_keeping_indices.append(all_snps.index(snps_pass_maf[idx]))\n",
    "    \n",
    "now_keeping_indices.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_keeping_indices.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/athaliana_small.snps.txt', 'w') as f:\n",
    "    f.write(\"%s\\n\" % \" \".join([all_snps[idx] for idx in now_keeping_indices]))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "with open('data/athaliana.raw') as f:\n",
    "    f.readline() # header\n",
    "    with open('data/athaliana_small.X.txt', 'w') as g:\n",
    "        for line in f:\n",
    "            samples.append(line.split()[0])\n",
    "            snps = np.array(line.split()[6:])\n",
    "            g.write(\"%s\\n\" % str.replace(\" \".join(snps[now_keeping_indices]), '2', '1'))\n",
    "    f.close()\n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/athaliana.samples.txt', 'w') as g:\n",
    "    g.write(\" \".join(samples))\n",
    "    g.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning phenotype data\n",
    "* For each of the phenotypes, regress out the 4 first principal components of the genetic data. This is a common pre-processing step to remove population structure effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For 2W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need the index in X, that is to say in samples, of the samples in 2W.pheno\n",
    "rows_2W = []\n",
    "y = []\n",
    "samples_2W = []\n",
    "with open('data/2W.pheno') as f:\n",
    "    f.readline() # header\n",
    "    for line in f:\n",
    "        sample_id = line.split()[1]\n",
    "        rows_2W.append(samples.index(sample_id))\n",
    "        y.append(float(line.split()[2]))\n",
    "    f.close()\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt('data/athaliana_small.X.txt', dtype='int')\n",
    "X = X[rows_2W, :]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check genomic inflation -- is there population structure?\n",
    "\n",
    "We compute p-values for the phenotype and check whether the distribution matches a uniform distribution. If not, it is likely due to confounding by population structure. We regress out PCs of the genotype data until the distribution is uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalues = []\n",
    "for feat_idx in range(X.shape[1]):\n",
    "    myX = X[:, feat_idx]\n",
    "    myX = sm.add_constant(myX)\n",
    "    est = sm.regression.linear_model.OLS(y, myX)\n",
    "    est2 = est.fit()\n",
    "    pvalues.append(est2.pvalues[1])\n",
    "pvalues = np.array(pvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ plot\n",
    "import scipy.stats as ss\n",
    "ss.probplot(pvalues, dist=\"uniform\", plot=plt)\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regress out principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=4)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_proj = X.dot(pca.components_.T)\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_proj, y)\n",
    "y_pred = model.predict(X_proj)\n",
    "y_res = y - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalues = []\n",
    "for feat_idx in range(X.shape[1]):\n",
    "    myX = X[:, feat_idx]\n",
    "    myX = sm.add_constant(myX)\n",
    "    est = sm.regression.linear_model.OLS(y_res, myX)\n",
    "    est2 = est.fit()\n",
    "    pvalues.append(est2.pvalues[1])\n",
    "pvalues = np.array(pvalues)\n",
    "\n",
    "# QQ plot\n",
    "import scipy.stats as ss\n",
    "ss.probplot(pvalues, dist=\"uniform\", plot=plt)\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/athaliana.2W.pheno', 'w') as f:\n",
    "    for ix, sample_id in enumerate([samples[ixx] for ixx in rows_2W]):\n",
    "        f.write('%s %.2f\\n' % (sample_id, y_res[ix]))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For 4W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need the index in X, that is to say in samples, of the samples in 2W.pheno\n",
    "rows_4W = []\n",
    "y = []\n",
    "samples_4W = []\n",
    "with open('data/4W.pheno') as f:\n",
    "    f.readline() # header\n",
    "    for line in f:\n",
    "        sample_id = line.split()[1]\n",
    "        rows_4W.append(samples.index(sample_id))\n",
    "        y.append(float(line.split()[2]))\n",
    "    f.close()\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt('data/athaliana_small.X.txt', dtype='int')\n",
    "X = X[rows_4W, :]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalues = []\n",
    "for feat_idx in range(X.shape[1]):\n",
    "    myX = X[:, feat_idx]\n",
    "    myX = sm.add_constant(myX)\n",
    "    est = sm.regression.linear_model.OLS(y, myX)\n",
    "    est2 = est.fit()\n",
    "    pvalues.append(est2.pvalues[1])\n",
    "pvalues = np.array(pvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ plot\n",
    "import scipy.stats as ss\n",
    "ss.probplot(pvalues, dist=\"uniform\", plot=plt)\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regress out principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=4)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_proj = X.dot(pca.components_.T)\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_proj, y)\n",
    "y_pred = model.predict(X_proj)\n",
    "y_res = y - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalues = []\n",
    "for feat_idx in range(X.shape[1]):\n",
    "    myX = X[:, feat_idx]\n",
    "    myX = sm.add_constant(myX)\n",
    "    est = sm.regression.linear_model.OLS(y_res, myX)\n",
    "    est2 = est.fit()\n",
    "    pvalues.append(est2.pvalues[1])\n",
    "pvalues = np.array(pvalues)\n",
    "\n",
    "# QQ plot\n",
    "import scipy.stats as ss\n",
    "ss.probplot(pvalues, dist=\"uniform\", plot=plt)\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/athaliana.4W.pheno', 'w') as f:\n",
    "    for ix, sample_id in enumerate([samples[ixx] for ixx in rows_4W]):\n",
    "        f.write('%s %.2f\\n' % (sample_id, y_res[ix]))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the network\n",
    "* Create a sparse matrix.\n",
    "* Create edges between all neighboring SNPs (in the genetic sequence), with a weight of 0.01.\n",
    "* Create edges between all pairs of SNPs that are in the same gene.\n",
    "* Create edges between all pairs of SNPs that are in two interacting genes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index of SNPs names in X\n",
    "snp_index = {} # snp_name: snp_index\n",
    "with open('data/athaliana_small.snps.txt') as f:\n",
    "    snps = f.readline().split()\n",
    "    f.close()\n",
    "\n",
    "for snp_ix, snp_name in enumerate(snps):\n",
    "    snp_index[snp_name] = snp_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = len(snps)\n",
    "print(p)\n",
    "W = sparse.lil_matrix(sparse.eye(p, k=1)*0.01) # connect each SNP to the next one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gene membership\n",
    "for snp_list in snps_by_genes.values():\n",
    "    if len(snp_list) <= 1:\n",
    "        break\n",
    "    for ix1, snp1 in enumerate(snp_list):\n",
    "        if snp_index.has_key(snp1):\n",
    "            # We kept that SNP\n",
    "            for snp2 in snp_list[ix1+1:]:\n",
    "                if snp_index.has_key(snp2):\n",
    "                    W[snp_index[snp1], snp_index[snp2]] = 1.\n",
    "                    W[snp_index[snp2], snp_index[snp1]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protein-protein interactions\n",
    "ppi_set = set([])\n",
    "with open('data/TairProteinInteraction.20090527.txt') as f:\n",
    "    f.readline() # header\n",
    "    for line in f:\n",
    "        ls = line.split()\n",
    "        if ls[0] < ls[2]:\n",
    "            ppi = (ls[0], ls[2])\n",
    "        else:\n",
    "            ppi = (ls[2], ls[0])\n",
    "        ppi_set.add(ppi)\n",
    "print(len(ppi_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ppi in ppi_set:\n",
    "    if snps_by_genes.has_key(ppi[0]):\n",
    "        for snp1 in snps_by_genes[ppi[0]]:        \n",
    "            if snp_index.has_key(snp1):\n",
    "                if snps_by_genes.has_key(ppi[1]):\n",
    "                    for snp2 in snps_by_genes[ppi[1]]:\n",
    "                        if snp_index.has_key(snp2):\n",
    "                            W[snp_index[snp1], snp_index[snp2]] = 1.\n",
    "                            W[snp_index[snp2], snp_index[snp1]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1087562./(19419*19419)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = sparse.coo_matrix(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([W.row, W.col, W.data])\n",
    "np.savetxt('data/athaliana_small.W.txt', A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "www_ = np.loadtxt('data/athaliana_small.W.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = sparse.coo_matrix((www_[2], (www_[0], www_[1])), shape=(p, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-create SNP by gene list to save as file\n",
    "Restricted to the SNPs included in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SNP names\n",
    "with open('data/athaliana_small.snps.txt') as f:\n",
    "    snp_names = f.readline().split()\n",
    "    f.close()\n",
    "print(len(snp_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromosomes = {}\n",
    "window = 20000\n",
    "with open('data/TAIR9_GFF3_genes.gff') as f:\n",
    "    for line in f:\n",
    "        ls = line.split()\n",
    "        if ls[2] == 'protein':\n",
    "            if ls[0] in ['ChrC', 'ChrM']:\n",
    "                continue\n",
    "            ccc = int(ls[0][3])\n",
    "            beg = int(ls[3])\n",
    "            end = int(ls[4])\n",
    "            protein_name = ls[8].split(\".\")[0][3:]\n",
    "            if not chromosomes.has_key(ccc):\n",
    "                chromosomes[ccc] = {}\n",
    "            chromosomes[ccc][(beg-window)] = [(end+window), protein_name]\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_begins = []\n",
    "chromosomes_ids = chromosomes.keys()\n",
    "chromosomes_ids.sort()\n",
    "print chromosomes_ids\n",
    "for ccc in chromosomes_ids:\n",
    "    begins = chromosomes[ccc].keys()\n",
    "    begins.sort()\n",
    "    print ccc, len(begins)\n",
    "    sorted_begins.append(begins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_snps = [] # list of indices (in snps_pass_maf) of candidate SNPs\n",
    "gene_snps = [] # list of indices (in snps_pass_maf) of SNPs in/near genes\n",
    "snps_in_genes = {} # key = SNP name, value = [index (in snps_pass_maf), set of proteins]\n",
    "snps_by_genes = {} # key = protein name, value = list of SNP names.\n",
    "\n",
    "for snp_idx, snp in enumerate(snp_names):\n",
    "    ccc = int(snp[3])\n",
    "    pos = int(snp.split(\"_\")[1])\n",
    "    begins = sorted_begins[(ccc-1)]\n",
    "    \n",
    "    # list all possible begin positions (those that are before pos)\n",
    "    possible_begs = []\n",
    "    for beg in begins:\n",
    "        if beg <= pos:\n",
    "            possible_begs.append(beg)\n",
    "        else:\n",
    "            break \n",
    "            \n",
    "    proteins = set([])\n",
    "    # reverse the list of possible begin positions\n",
    "    possible_begs.reverse()\n",
    "    for beg in possible_begs:\n",
    "        # look for end positions that are after pos\n",
    "        end, protein_name = chromosomes[ccc][beg]\n",
    "        if end >= pos:\n",
    "            if not snps_in_genes.has_key(snp):\n",
    "                snps_in_genes[snp] = [snp_idx]\n",
    "            proteins.add(protein_name)\n",
    "        else:\n",
    "            # now both begin and end are before pos, stop looking\n",
    "            break\n",
    "                \n",
    "    if snps_in_genes.has_key(snp):\n",
    "        # this is not an intergenic gene, keep processing\n",
    "        snps_in_genes[snp].append(proteins)\n",
    "        for protein_name in proteins:\n",
    "            if not snps_by_genes.has_key(protein_name):\n",
    "                snps_by_genes[protein_name] = []\n",
    "            snps_by_genes[protein_name].append(snp)\n",
    "                \n",
    "        if len(proteins.intersection(candidate_genes)):\n",
    "            cand_snps.append(snp_idx)\n",
    "        else:\n",
    "            gene_snps.append(snp_idx)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/athaliana.snps_by_gene.txt', 'w') as f:\n",
    "    for gene, snps_list in snps_by_genes.iteritems():\n",
    "        f.write(\"%s %s\\n\" % (gene, \" \".join(snps_list)))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of candidate genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File('data/arabidopsis_processed_data/FT_cand_genes.hd5', 'r')\n",
    "candidate_genes = list(f['gene_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/athaliana.candidates.txt', 'w') as f:\n",
    "    f.write(\"%s\\n\" % \" \".join(candidate_genes))\n",
    "    f.close"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
